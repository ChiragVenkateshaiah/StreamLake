# Kafka-First Streaming Ingestion & Lakehouse Platform (with Airflow Orchestration)

## Objective
1. What exactly are we building?
2. What are we explicitly not building?
3. How Kafka and Airflow coexist correctly?

## Final Problem Statement (Locked)

> Build a Kafka-first streaming ingestion & lakehouse platform where Kafka is the system of record for ingestion, Airflow orchestrates operational workflows (replay, backfill, checks), and data lands in a lakehouse-style raw zone on GCP.

Inspiration: OpenLakeTx(transactions), LakeGuard(governance)

## Architecture North-Star Principles
ğŸ”’ Principle 1: Kafka is the system of record
- All replay originates from Kafka
- Storage is downstream & replacable
- Failures are solved via reprocessing, not overwrites

ğŸ”’ Principle 2: Streaming is not equal to Orchestration
- Kafka -> Event-driven, continuous
- Airflow -> Batch, control-plane, operational
- They must not overlap responsibilities

ğŸ”’ Principle 3: Thin Edges, Strong Core
- API is thin
- Kafka is deep
- Consumers are reliable
- Orchestration is explicit

These principles are industry-correct.

## Final High-Level Architecture
```
External Producers
        â†“
Thin Ingestion API (FastAPI)
        â†“
Apache Kafka
  â”œâ”€â”€ RAW Topics
  â””â”€â”€ DLQ Topics
        â†“
Long-Running Kafka Consumers
        â†“
GCS Lakehouse (Raw Zone)
            â†‘
       Apache Airflow
 (Replay / Backfill / Ops / DQ)
```

## 5. Component Responsibilities

### 1. Think Ingestion API (FastAPI)
Purpose:
- Accept JSON events
- Validate schema (Pydantic)
- Add metadata
- Produce to Kafka

Explicitly NOT doing:
- âŒTransformations
- âŒAggregations
- âŒState management
- âŒScheduling

> API exists to feed Kafka, not replace it.

### 2. Streaming Backbone -- Apache Kafka (CORE)
Responsibilities:
- Topic partitioning
- Ordering guarantees (within partition)
- At-least-once delivery
- Consumer groups
- Manual offset commits
- Replay (offset & timestamp)
- DLQ handling

### 3. Lakehouse Raw Zone - GCS
Purpose:
- Durable storage
- Auditability
- Analytics readiness

Layout:
```perl
gs://<bucket>/
 â””â”€â”€ raw/
     â””â”€â”€ dataset=<name>/
         â””â”€â”€ ingestion_date=YYYY-MM-DD/
             â”œâ”€â”€ data.json / parquet
             â””â”€â”€ _metadata.json
```
No curated zone in this project

### 4. Failure Handling (Mandatory)
**Failures we MUST handle:**
- Schema mismatch
- Poison pill records
- Consumer crash
- Storage write failure

How:
- DLQ Kafka topics
- Error metadata in headers
- Replay via Kafka offsets


### 5. Airflow (Orchestration Layer -- Explicit Scope)
What Airflow WILL Do
- âœ” Trigger Kafka replay jobs
- âœ” Run backfills (date / offset range)
- âœ” Perform batch data-quality checks
- âœ” Inspect DLQ topics
- âœ” Generate audit / ops summaries

What Airflow WILL NOT Do
- âŒ Run streaming consumers
- âŒ Schedule ingestion
- âŒ Replace Kafka semantics

> Airflow controls actions around Kafka, not Kafka itself.

### 6. Explicit Non-Goals
We are NOT building:
- âŒ UI dashboards
- âŒ Managed Kafka / PubSub
- âŒ Spark / Flink jobs
- âŒ Delta / Iceberg engines
- âŒ CDC pipelines
- âŒ Full governance engine
- âŒ Auth systems (OAuth, RBAC)

These are intentional exclusions, not missing features.

### 7. Technology Stack
| Layer         | Tool                         |
| ------------- | ---------------------------- |
| Streaming     | Apache Kafka                 |
| Orchestration | Apache Airflow (later phase) |
| API           | FastAPI                      |
| Validation    | Pydantic                     |
| Storage       | GCS                          |
| Runtime       | Docker                       |
| Language      | Python                       |
| Compute       | GCP VM                       |

All open source + cloud

### 8. Data Guarantees
- Delivery: At-least-once
- Ordering: Within partition
- Idempotency: Best-effort consumer writes
- Replay: Offset-based + timestamp-based
- Failure isolation: DLQ topics

### 9. Repository Structure
```
streaming-lakehouse/
 â”œâ”€â”€ api/
 â”œâ”€â”€ kafka/
 â”‚   â”œâ”€â”€ producer/
 â”‚   â”œâ”€â”€ consumer/
 â”‚   â””â”€â”€ topics.py
 â”œâ”€â”€ airflow/
 â”‚   â””â”€â”€ dags/
 â”œâ”€â”€ storage/
 â”‚   â””â”€â”€ gcs/
 â”œâ”€â”€ contracts/
 â”œâ”€â”€ scripts/
 â”œâ”€â”€ docs/
 â””â”€â”€ docker-compose.yml
```

